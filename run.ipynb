{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0d43f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: mps\n",
      "\n",
      "ðŸ§© Running LSTM with seq_len=10, num_layer=1, hidden_unit=64\n",
      "\n",
      "ðŸš€ Running LSTM model (seq_len=10)...\n",
      "ðŸ“Š Dataset split: Train=84844, Val=9981, Test=11005\n",
      "âœ… DataLoaders ready: Train=1326 batches, Val=156 batches, Test=172 batches\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::linalg_qr.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ§© Running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with seq_len=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_layer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_unit=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_unit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 30\u001b[0m acc, prec, rec \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_unit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     32\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeqLen\u001b[39m\u001b[38;5;124m\"\u001b[39m: seq_len,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime (s)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(end \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m })\n",
      "File \u001b[0;32m~/Documents/Learning/25fallLesson/CSE5526/Homeworks/pa2/experiments.py:265\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(model_name, n_features, hidden_size, layers, seq_len, snr_db)\u001b[0m\n\u001b[1;32m    262\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(model_name, n_features\u001b[38;5;241m=\u001b[39mn_features, hidden_size\u001b[38;5;241m=\u001b[39mhidden_size, num_layers\u001b[38;5;241m=\u001b[39mlayers)\n\u001b[1;32m    263\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 265\u001b[0m model, hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    268\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Learning/25fallLesson/CSE5526/Homeworks/pa2/experiments.py:75\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, val_dl, epochs, lr, patience)\u001b[0m\n\u001b[1;32m     73\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Apply Xavier init to all layers\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_weights_xavier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# ---- Training ----\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearningdude/lib/python3.10/site-packages/torch/nn/modules/module.py:1033\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \n\u001b[1;32m    999\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1033\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearningdude/lib/python3.10/site-packages/torch/nn/modules/module.py:1034\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   1033\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m-> 1034\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Learning/25fallLesson/CSE5526/Homeworks/pa2/models.py:49\u001b[0m, in \u001b[0;36minit_weights_xavier\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     46\u001b[0m     nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mxavier_uniform_(param)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_hh\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Recommended for recurrent matrices\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morthogonal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m     51\u001b[0m     nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mzeros_(param)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearningdude/lib/python3.10/site-packages/torch/nn/init.py:610\u001b[0m, in \u001b[0;36morthogonal_\u001b[0;34m(tensor, gain, generator)\u001b[0m\n\u001b[1;32m    607\u001b[0m     flattened\u001b[38;5;241m.\u001b[39mt_()\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Compute the qr factorization\u001b[39;00m\n\u001b[0;32m--> 610\u001b[0m q, r \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\u001b[39;00m\n\u001b[1;32m    612\u001b[0m d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(r, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::linalg_qr.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CSE 5526 - Programming Assignment 2\n",
    "# Voice Activity Detection in Noise using Deep Learning\n",
    "# ============================================================\n",
    "\n",
    "# --- Imports ---\n",
    "from experiments import run_experiment\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torchaudio.*backend.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torchcodec.*AudioDecoder.*\", category=UserWarning)\n",
    "\n",
    "# ============================================================\n",
    "# run.ipynb\n",
    "# ============================================================\n",
    "\n",
    "from experiments import run_experiment\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "results = []\n",
    "\n",
    "for seq_len in [10, 25, 50, 100]:\n",
    "    for num_layer in [1, 2, 3]:\n",
    "        for hidden_unit in [64, 128, 256]:\n",
    "            for model_name in [\"lstm\", \"bilstm\", \"cnnlstm\"]:\n",
    "                print(f\"\\nðŸ§© Running {model_name.upper()} with seq_len={seq_len}, num_layer={num_layer}, hidden_unit={hidden_unit}\")\n",
    "                start = time.time()\n",
    "                acc, prec, rec = run_experiment(model_name, n_features=40, hidden_size=hidden_unit, layers=num_layer, seq_len=seq_len)\n",
    "                end = time.time()\n",
    "                results.append({\n",
    "                    \"Model\": model_name.upper(),\n",
    "                    \"SeqLen\": seq_len,\n",
    "                    \"HiddenUnit\": hidden_unit,\n",
    "                    \"Layer\": num_layer,\n",
    "                    \"Accuracy\": acc,\n",
    "                    \"Precision\": prec,\n",
    "                    \"Recall\": rec,\n",
    "                    \"Time (s)\": round(end - start, 1)\n",
    "                })\n",
    "\n",
    "# Display results as table\n",
    "df = pd.DataFrame(results)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningdude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
